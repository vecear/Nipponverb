
import re

target_files = [
    'src/data/details/n4.ts',
    'src/data/questions/n4.ts'
]

# N4 Vocabulary Mapping
vocab_list = {
    # Part 1: Observed from Audit (Top 50 Missing)
    "勉強": "勉{べん}強{きょう}",
    "一緒": "一{いっ}緒{しょ}",
    "予約": "予{よ}約{やく}",
    "出席": "出{しゅっ}席{せき}",
    "口語": "口{こう}語{ご}",
    "季節": "季{き}節{せつ}",
    "週末": "週{しゅう}末{まつ}",
    "形容詞": "形{けい}容{よう}詞{し}",
    "動詞": "動{どう}詞{し}",
    "名詞": "名{めい}詞{し}",
    "食": "食{た}", # Risky? Usually Taberu. But "shoku" is also possible. Better stick to compounds or unambiguous.
    "食べ": "食{た}べ",
    "飲み": "飲{の}み",
    "来ま": "来{き}ま", 
    "来る": "来{く}る", 
    "来ない": "来{こ}ない", 
    "待ち": "待{ま}ち",
    "持ち": "持{も}ち",
    "買い": "買{か}い",
    "習い": "習{なら}い",
    "描き": "描{か}き",
    "走り": "走{はし}り",
    "願い": "願{ねが}い", 
    "遅い": "遅{おそ}い", 
    "遅れ": "遅{おく}れ",
    "知っ": "知{し}っ", 
    "雨": "雨{あめ}",
    "猫": "猫{ねこ}",
    "犬": "犬{いぬ}",
    "味": "味{あじ}",
    "席": "席{せき}",
    "晴": "晴{は}れ", 
    "負": "負{ま}け", 
    "面白": "面{おも}白{しろ}", 
    "暑い": "暑{あつ}い",
    "重い": "重{おも}い", 
    "寒い": "寒{さむ}い",
    
    # Part 2: Restore Original N4 High Frequency
    "説明": "説{せつ}明{めい}",
    "利用": "利{り}用{よう}",
    "案内": "案{あん}内{ない}",
    "紹介": "紹{しょう}介{かい}",
    "連絡": "連{れん}絡{らく}",
    "相談": "相{そう}談{だん}",
    "約束": "約{やく}束{そく}",
    "準備": "準{じゅん}備{び}",
    "予習": "予{よ}習{しゅう}",
    "復習": "復{ふく}習{しゅう}",
    "招待": "招{しょう}待{たい}",
    "翻訳": "翻{ほん}訳{やく}",
    "理由": "理{り}由{ゆう}",
    "原因": "原{げん}因{いん}",
    "目的": "目{もく}的{てき}",
    "意見": "意{い}見{けん}",
    "興味": "興{きょう}味{み}",
    "最近": "最{さい}近{きん}",
    "最初": "最{さい}初{しょ}",
    "最後": "最{さい}後{ご}",
    "場合": "場{ば}合{あい}",
    "機会": "機{き}会{かい}",
    "経験": "経{けい}験{けん}",
    "習慣": "習{しゅう}慣{かん}",
    "社会": "社{しゃ}会{かい}",
    "世界": "世{せ}界{かい}",
    "政治": "政{せい}治{じ}",
    "経済": "経{けい}済{ざい}",
    "法律": "法{ほう}律{りつ}",
    "教育": "教{きょう}育{いく}",
    "文化": "文{ぶん}化{か}",
    "歴史": "歴{れき}史{し}",
    "将来": "将{しょう}来{らい}",
    "夢": "夢{ゆめ}",
    "事務所": "事{じ}務{む}所{しょ}",
    "会場": "会{かい}場{じょう}",
    "工場": "工{こう}場{じょう}",
    "美術館": "美{び}術{じゅ}館{かん}",
    "動物園": "動{どう}物{ぶつ}園{えん}",
    "空港": "空{くう}港{こう}",
    "港": "港{みなと}",
    "男性": "男{だん}性{せい}",
    "女性": "女{じょ}性{せい}",
    "彼": "彼{かれ}",
    "彼女": "彼{かの}女{じょ}",
    "先輩": "先{せん}輩{ぱい}",
    "後輩": "後{こう}輩{はい}",
    "相手": "相{あい}手{て}",
    "客": "客{きゃく}",
    "簡単": "簡{かん}単{たん}",
    "複雑": "複{ふく}雑{ざつ}",
    "便利": "便{べん}利{り}",
    "不便": "不{ふ}便{べん}",
    "危険": "危{き}険{けん}",
    "安全": "安{あん}全{ぜん}",
    "丁寧": "丁{てい}寧{ねい}",
    "自由": "自{じ}由{ゆう}",
    "十分": "十{じゅう}分{ぶん}",
    "大好き": "大{だい}好{す}き",
    "大嫌い": "大{だい}嫌{きら}い",
    "今週": "今{こん}週{しゅう}",
    "来週": "来{らい}週{しゅう}",
    "先週": "先{せん}週{しゅう}",
    "今年": "今{こ}年{とし}",
    "来年": "来{らい}年{ねん}",
    "去年": "去{きょ}年{ねん}",
    "昔": "昔{むかし}",
}

def add_furigana():
    total_added = 0
    
    for file_path in target_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        original_content = content
        
        for word, annotated in vocab_list.items():
            # Regex: Word(?![{]) matches Word NOT followed by {
            # Be precise.
            pattern = re.escape(word) + r"(?![{])"
            
            # We use re.subn to count replacements
            content, count = re.subn(pattern, annotated, content)
            total_added += count
            
        if content != original_content:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"Updated {file_path}")
            
    print(f"Total annotations added: {total_added}")

if __name__ == "__main__":
    add_furigana()
